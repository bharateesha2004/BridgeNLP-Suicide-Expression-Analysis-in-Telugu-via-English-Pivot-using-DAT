{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11155856,"sourceType":"datasetVersion","datasetId":6960412},{"sourceId":11155896,"sourceType":"datasetVersion","datasetId":6960442},{"sourceId":303237,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":258898,"modelId":280093}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n# chatGPT\n\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nimport os\nimport random\nfrom itertools import cycle\nimport time\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Set random seed\ntorch.manual_seed(42)\ntorch.backends.cudnn.benchmark = True  # Optimize GPU performance\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n\n# Load datasets\nenglish_df = pd.read_csv(\"/kaggle/input/english-ds-binary/BinaryCsv_English_dataset_UTF8.csv\", encoding='utf-8')\ntelugu_df = pd.read_csv(\"/kaggle/input/telugu-ds-binary/Binary_telugu_dataset.csv\", encoding='utf-8')\n\n# Add domain labels\nenglish_df['domain'] = 0\ntelugu_df['domain'] = 1\n\n# Combine datasets\nfull_df = pd.concat([english_df, telugu_df])\n\n# Split into train and validation sets\ntrain_df, val_df = train_test_split(full_df, test_size=0.2, stratify=full_df[['class', 'domain']])\n\n# Custom Dataset class\nclass SuicideDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=128):\n        self.texts = dataframe['text'].tolist()\n        self.labels = dataframe['class'].tolist()\n        self.domains = dataframe['domain'].tolist()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        domain = self.domains[idx]\n        encoding = self.tokenizer(\n            text, return_tensors='pt', max_length=self.max_length,\n            truncation=True, padding='max_length'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(label, dtype=torch.long),\n            'domains': torch.tensor(domain, dtype=torch.long)\n        }\n\n# Balanced Batch Sampler\nclass BalancedBatchSampler(Sampler):\n    def __init__(self, dataset, batch_size_per_lang):\n        self.eng_indices = [i for i, domain in enumerate(dataset.domains) if domain == 0]\n        self.tel_indices = [i for i, domain in enumerate(dataset.domains) if domain == 1]\n        self.batch_size_per_lang = batch_size_per_lang\n        self.total_batches = len(self.eng_indices) // batch_size_per_lang\n\n    def __iter__(self):\n        random.shuffle(self.eng_indices)\n        tel_iter = cycle(self.tel_indices)\n        for i in range(self.total_batches):\n            eng_batch = self.eng_indices[i * self.batch_size_per_lang:(i + 1) * self.batch_size_per_lang]\n            tel_batch = [next(tel_iter) for _ in range(self.batch_size_per_lang)]\n            yield eng_batch + tel_batch\n\n    def __len__(self):\n        return self.total_batches\n\n# Gradient Reversal Layer\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\n# DAT Model\nclass DATModel(nn.Module):\n    def __init__(self, base_model, hidden_size=768, num_classes=2, num_domains=2):\n        super(DATModel, self).__init__()\n        self.base_model = base_model\n        self.task_classifier = nn.Linear(hidden_size, num_classes)\n        self.domain_classifier = nn.Linear(hidden_size, num_domains)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, input_ids, attention_mask, lambda_=1.0):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        features = outputs.last_hidden_state[:, 0, :]\n        features = self.dropout(features)\n        task_logits = self.task_classifier(features)\n        domain_features = GradientReversal.apply(features, lambda_)\n        domain_logits = self.domain_classifier(domain_features)\n        return task_logits, domain_logits\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nbase_model = AutoModel.from_pretrained(\"google/muril-base-cased\")\nmodel = DATModel(base_model).to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\n# Create datasets\ntrain_dataset = SuicideDataset(train_df, tokenizer)\nval_dataset = SuicideDataset(val_df, tokenizer)\n\n# DataLoader settings\nbatch_size_per_lang = 64  # 32 English + 32 Telugu = 64 total batch size\ntrain_sampler = BalancedBatchSampler(train_dataset, batch_size_per_lang)\ntrain_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\n# Loss functions\ncriterion_task = nn.CrossEntropyLoss()\ncriterion_domain = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# AMP (Mixed Precision Training)\nscaler = GradScaler()\n\n# Training loop\nnum_epochs = 3\nlambda_ = 1.0\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_batches = len(train_loader)\n    \n    # Initialize epoch metrics\n    total_task_loss = 0.0\n    total_domain_loss = 0.0\n    total_correct_task = 0\n    total_samples = 0\n    start_time = time.time()\n    \n    for batch_idx, batch in enumerate(train_loader, 1):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        domains = batch['domains'].to(device)\n\n        optimizer.zero_grad()\n\n        with autocast():\n            task_logits, domain_logits = model(input_ids, attention_mask, lambda_)\n            task_loss = criterion_task(task_logits, labels)\n            domain_loss = criterion_domain(domain_logits, domains)\n            total_loss = task_loss + lambda_ * domain_loss\n\n        # Scale loss and backpropagate\n        scaler.scale(total_loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Update epoch metrics\n        total_task_loss += task_loss.item()\n        total_domain_loss += domain_loss.item()\n        predictions = torch.argmax(task_logits, dim=1)\n        total_correct_task += torch.sum(predictions == labels).item()\n        total_samples += labels.size(0)\n\n        # Print metrics every 100 batches\n        if batch_idx % 100 == 0:\n            progress = (batch_idx / total_batches) * 100\n            print(f\"Epoch {epoch+1}, Batch {batch_idx}/{total_batches} ({progress:.2f}%), \"\n                  f\"Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}, \"\n                  f\"Task Accuracy: {(predictions == labels).float().mean():.4f}\")\n\n    # Epoch summary metrics\n    epoch_task_loss = total_task_loss / len(train_loader)\n    epoch_domain_loss = total_domain_loss / len(train_loader)\n    epoch_task_accuracy = total_correct_task / total_samples\n    epoch_time = time.time() - start_time\n    print(f\"Epoch {epoch+1} Completed: Task Loss: {epoch_task_loss:.4f}, \"\n          f\"Domain Loss: {epoch_domain_loss:.4f}, Task Accuracy: {epoch_task_accuracy:.4f}, \"\n          f\"Time: {epoch_time:.2f}s\")\n\n    # Overall Validation metrics calculation (on entire 20% split)\n    model.eval()\n    val_task_loss = 0.0\n    val_domain_loss = 0.0\n    val_correct = 0\n    val_samples = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            domains = batch['domains'].to(device)\n            \n            with autocast():\n                task_logits, domain_logits = model(input_ids, attention_mask, lambda_)\n                loss_task = criterion_task(task_logits, labels)\n                loss_domain = criterion_domain(domain_logits, domains)\n            \n            val_task_loss += loss_task.item()\n            val_domain_loss += loss_domain.item()\n            predictions = torch.argmax(task_logits, dim=1)\n            val_correct += torch.sum(predictions == labels).item()\n            val_samples += labels.size(0)\n    \n    val_task_loss /= len(val_loader)\n    val_domain_loss /= len(val_loader)\n    val_accuracy = val_correct / val_samples\n    print(f\"Validation (Overall) - Task Loss: {val_task_loss:.4f}, Domain Loss: {val_domain_loss:.4f}, \"\n          f\"Task Accuracy: {val_accuracy:.4f}\")\n\n    # Telugu-specific Validation metrics calculation\n    # Filter the validation DataFrame for Telugu samples (domain==1)\n    telugu_val_df = val_df[val_df['domain'] == 1]\n    telugu_val_dataset = SuicideDataset(telugu_val_df, tokenizer)\n    telugu_val_loader = DataLoader(telugu_val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n    \n    telugu_val_task_loss = 0.0\n    telugu_val_domain_loss = 0.0\n    telugu_val_correct = 0\n    telugu_val_samples = 0\n    with torch.no_grad():\n        for batch in telugu_val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            domains = batch['domains'].to(device)\n            \n            with autocast():\n                task_logits, domain_logits = model(input_ids, attention_mask, lambda_)\n                loss_task = criterion_task(task_logits, labels)\n                loss_domain = criterion_domain(domain_logits, domains)\n            \n            telugu_val_task_loss += loss_task.item()\n            telugu_val_domain_loss += loss_domain.item()\n            predictions = torch.argmax(task_logits, dim=1)\n            telugu_val_correct += torch.sum(predictions == labels).item()\n            telugu_val_samples += labels.size(0)\n    \n    telugu_val_task_loss /= len(telugu_val_loader)\n    telugu_val_domain_loss /= len(telugu_val_loader)\n    telugu_val_accuracy = telugu_val_correct / telugu_val_samples\n    print(f\"Validation (Telugu Only) - Task Loss: {telugu_val_task_loss:.4f}, Domain Loss: {telugu_val_domain_loss:.4f}, \"\n          f\"Task Accuracy: {telugu_val_accuracy:.4f}\")\n\n    # Save model checkpoint\n    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T06:09:29.142724Z","iopub.execute_input":"2025-03-26T06:09:29.143068Z","iopub.status.idle":"2025-03-26T07:45:13.797679Z","shell.execute_reply.started":"2025-03-26T06:09:29.143041Z","shell.execute_reply":"2025-03-26T07:45:13.796879Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing 2 GPUs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b0e44c9851e44abbe80ad6bff99e5f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34b5cc8196f14d5494dcedc95b7c7f25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3240bcc2db44ab9e1e926b071d04ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f8830080a64b92a66e43dca7e50fda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633efe6fe1d34b2b9522ab5d4bab2c42"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-1-50528efe8228>:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n<ipython-input-1-50528efe8228>:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 100/2916 (3.43%), Task Loss: 0.6274, Domain Loss: 0.8337, Task Accuracy: 0.7969\nEpoch 1, Batch 200/2916 (6.86%), Task Loss: 0.5736, Domain Loss: 0.8170, Task Accuracy: 0.8516\nEpoch 1, Batch 300/2916 (10.29%), Task Loss: 0.4981, Domain Loss: 0.7931, Task Accuracy: 0.9453\nEpoch 1, Batch 400/2916 (13.72%), Task Loss: 0.4731, Domain Loss: 0.7795, Task Accuracy: 0.9141\nEpoch 1, Batch 500/2916 (17.15%), Task Loss: 0.4030, Domain Loss: 0.7628, Task Accuracy: 0.9453\nEpoch 1, Batch 600/2916 (20.58%), Task Loss: 0.3735, Domain Loss: 0.7543, Task Accuracy: 0.9297\nEpoch 1, Batch 700/2916 (24.01%), Task Loss: 0.3282, Domain Loss: 0.7429, Task Accuracy: 0.9219\nEpoch 1, Batch 800/2916 (27.43%), Task Loss: 0.3229, Domain Loss: 0.7331, Task Accuracy: 0.9219\nEpoch 1, Batch 900/2916 (30.86%), Task Loss: 0.2317, Domain Loss: 0.7188, Task Accuracy: 0.9531\nEpoch 1, Batch 1000/2916 (34.29%), Task Loss: 0.2239, Domain Loss: 0.7156, Task Accuracy: 0.9531\nEpoch 1, Batch 1100/2916 (37.72%), Task Loss: 0.1568, Domain Loss: 0.7044, Task Accuracy: 0.9844\nEpoch 1, Batch 1200/2916 (41.15%), Task Loss: 0.1820, Domain Loss: 0.7036, Task Accuracy: 0.9688\nEpoch 1, Batch 1300/2916 (44.58%), Task Loss: 0.2426, Domain Loss: 0.6987, Task Accuracy: 0.9219\nEpoch 1, Batch 1400/2916 (48.01%), Task Loss: 0.1500, Domain Loss: 0.6952, Task Accuracy: 0.9688\nEpoch 1, Batch 1500/2916 (51.44%), Task Loss: 0.1856, Domain Loss: 0.6936, Task Accuracy: 0.9297\nEpoch 1, Batch 1600/2916 (54.87%), Task Loss: 0.1782, Domain Loss: 0.6927, Task Accuracy: 0.9453\nEpoch 1, Batch 1700/2916 (58.30%), Task Loss: 0.1257, Domain Loss: 0.6928, Task Accuracy: 0.9531\nEpoch 1, Batch 1800/2916 (61.73%), Task Loss: 0.1464, Domain Loss: 0.6929, Task Accuracy: 0.9688\nEpoch 1, Batch 1900/2916 (65.16%), Task Loss: 0.2412, Domain Loss: 0.6938, Task Accuracy: 0.9141\nEpoch 1, Batch 2000/2916 (68.59%), Task Loss: 0.1389, Domain Loss: 0.6937, Task Accuracy: 0.9531\nEpoch 1, Batch 2100/2916 (72.02%), Task Loss: 0.1736, Domain Loss: 0.6930, Task Accuracy: 0.9375\nEpoch 1, Batch 2200/2916 (75.45%), Task Loss: 0.1008, Domain Loss: 0.6940, Task Accuracy: 0.9609\nEpoch 1, Batch 2300/2916 (78.88%), Task Loss: 0.0419, Domain Loss: 0.6934, Task Accuracy: 1.0000\nEpoch 1, Batch 2400/2916 (82.30%), Task Loss: 0.0817, Domain Loss: 0.6931, Task Accuracy: 0.9844\nEpoch 1, Batch 2500/2916 (85.73%), Task Loss: 0.1709, Domain Loss: 0.6933, Task Accuracy: 0.9453\nEpoch 1, Batch 2600/2916 (89.16%), Task Loss: 0.1555, Domain Loss: 0.6933, Task Accuracy: 0.9609\nEpoch 1, Batch 2700/2916 (92.59%), Task Loss: 0.0515, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 1, Batch 2800/2916 (96.02%), Task Loss: 0.0772, Domain Loss: 0.6929, Task Accuracy: 0.9766\nEpoch 1, Batch 2900/2916 (99.45%), Task Loss: 0.1103, Domain Loss: 0.6930, Task Accuracy: 0.9609\nEpoch 1 Completed: Task Loss: 0.2391, Domain Loss: 0.7201, Task Accuracy: 0.9449, Time: 1754.76s\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-50528efe8228>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Validation (Overall) - Task Loss: 0.1029, Domain Loss: 0.6933, Task Accuracy: 0.9651\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-50528efe8228>:248: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Validation (Telugu Only) - Task Loss: 0.1848, Domain Loss: 0.6930, Task Accuracy: 0.9318\nEpoch 2, Batch 100/2916 (3.43%), Task Loss: 0.0664, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 2, Batch 200/2916 (6.86%), Task Loss: 0.0361, Domain Loss: 0.6932, Task Accuracy: 0.9922\nEpoch 2, Batch 300/2916 (10.29%), Task Loss: 0.0866, Domain Loss: 0.6930, Task Accuracy: 0.9609\nEpoch 2, Batch 400/2916 (13.72%), Task Loss: 0.1068, Domain Loss: 0.6933, Task Accuracy: 0.9609\nEpoch 2, Batch 500/2916 (17.15%), Task Loss: 0.1034, Domain Loss: 0.6926, Task Accuracy: 0.9766\nEpoch 2, Batch 600/2916 (20.58%), Task Loss: 0.0625, Domain Loss: 0.6933, Task Accuracy: 0.9922\nEpoch 2, Batch 700/2916 (24.01%), Task Loss: 0.0245, Domain Loss: 0.6927, Task Accuracy: 1.0000\nEpoch 2, Batch 800/2916 (27.43%), Task Loss: 0.1456, Domain Loss: 0.6932, Task Accuracy: 0.9453\nEpoch 2, Batch 900/2916 (30.86%), Task Loss: 0.0633, Domain Loss: 0.6922, Task Accuracy: 0.9766\nEpoch 2, Batch 1000/2916 (34.29%), Task Loss: 0.0869, Domain Loss: 0.6933, Task Accuracy: 0.9766\nEpoch 2, Batch 1100/2916 (37.72%), Task Loss: 0.0252, Domain Loss: 0.6929, Task Accuracy: 0.9922\nEpoch 2, Batch 1200/2916 (41.15%), Task Loss: 0.1163, Domain Loss: 0.6930, Task Accuracy: 0.9531\nEpoch 2, Batch 1300/2916 (44.58%), Task Loss: 0.1070, Domain Loss: 0.6936, Task Accuracy: 0.9688\nEpoch 2, Batch 1400/2916 (48.01%), Task Loss: 0.0430, Domain Loss: 0.6936, Task Accuracy: 0.9844\nEpoch 2, Batch 1500/2916 (51.44%), Task Loss: 0.0607, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 2, Batch 1600/2916 (54.87%), Task Loss: 0.0695, Domain Loss: 0.6932, Task Accuracy: 0.9688\nEpoch 2, Batch 1700/2916 (58.30%), Task Loss: 0.0232, Domain Loss: 0.6946, Task Accuracy: 0.9922\nEpoch 2, Batch 1800/2916 (61.73%), Task Loss: 0.0684, Domain Loss: 0.6939, Task Accuracy: 0.9844\nEpoch 2, Batch 1900/2916 (65.16%), Task Loss: 0.1078, Domain Loss: 0.6930, Task Accuracy: 0.9688\nEpoch 2, Batch 2000/2916 (68.59%), Task Loss: 0.0683, Domain Loss: 0.6920, Task Accuracy: 0.9766\nEpoch 2, Batch 2100/2916 (72.02%), Task Loss: 0.1213, Domain Loss: 0.6932, Task Accuracy: 0.9688\nEpoch 2, Batch 2200/2916 (75.45%), Task Loss: 0.0658, Domain Loss: 0.6934, Task Accuracy: 0.9766\nEpoch 2, Batch 2300/2916 (78.88%), Task Loss: 0.0367, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 2, Batch 2400/2916 (82.30%), Task Loss: 0.0335, Domain Loss: 0.6933, Task Accuracy: 0.9922\nEpoch 2, Batch 2500/2916 (85.73%), Task Loss: 0.1378, Domain Loss: 0.6940, Task Accuracy: 0.9531\nEpoch 2, Batch 2600/2916 (89.16%), Task Loss: 0.0721, Domain Loss: 0.6929, Task Accuracy: 0.9844\nEpoch 2, Batch 2800/2916 (96.02%), Task Loss: 0.0438, Domain Loss: 0.6948, Task Accuracy: 0.9922\nEpoch 2, Batch 2900/2916 (99.45%), Task Loss: 0.0676, Domain Loss: 0.6935, Task Accuracy: 0.9766\nEpoch 2 Completed: Task Loss: 0.0705, Domain Loss: 0.6932, Task Accuracy: 0.9798, Time: 1757.53s\nValidation (Overall) - Task Loss: 0.1010, Domain Loss: 0.6950, Task Accuracy: 0.9677\nValidation (Telugu Only) - Task Loss: 0.2440, Domain Loss: 0.6901, Task Accuracy: 0.9310\nEpoch 3, Batch 100/2916 (3.43%), Task Loss: 0.0179, Domain Loss: 0.6930, Task Accuracy: 0.9922\nEpoch 3, Batch 200/2916 (6.86%), Task Loss: 0.0078, Domain Loss: 0.6930, Task Accuracy: 1.0000\nEpoch 3, Batch 300/2916 (10.29%), Task Loss: 0.0263, Domain Loss: 0.6937, Task Accuracy: 0.9922\nEpoch 3, Batch 400/2916 (13.72%), Task Loss: 0.0140, Domain Loss: 0.6929, Task Accuracy: 1.0000\nEpoch 3, Batch 500/2916 (17.15%), Task Loss: 0.0883, Domain Loss: 0.6931, Task Accuracy: 0.9766\nEpoch 3, Batch 600/2916 (20.58%), Task Loss: 0.0395, Domain Loss: 0.6936, Task Accuracy: 0.9922\nEpoch 3, Batch 700/2916 (24.01%), Task Loss: 0.0197, Domain Loss: 0.6930, Task Accuracy: 0.9922\nEpoch 3, Batch 800/2916 (27.43%), Task Loss: 0.0685, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 3, Batch 900/2916 (30.86%), Task Loss: 0.0057, Domain Loss: 0.6933, Task Accuracy: 1.0000\nEpoch 3, Batch 1000/2916 (34.29%), Task Loss: 0.0108, Domain Loss: 0.6932, Task Accuracy: 1.0000\nEpoch 3, Batch 1100/2916 (37.72%), Task Loss: 0.0197, Domain Loss: 0.6934, Task Accuracy: 0.9922\nEpoch 3, Batch 1200/2916 (41.15%), Task Loss: 0.0309, Domain Loss: 0.6933, Task Accuracy: 0.9844\nEpoch 3, Batch 1300/2916 (44.58%), Task Loss: 0.0929, Domain Loss: 0.6934, Task Accuracy: 0.9688\nEpoch 3, Batch 1400/2916 (48.01%), Task Loss: 0.0823, Domain Loss: 0.6930, Task Accuracy: 0.9844\nEpoch 3, Batch 1500/2916 (51.44%), Task Loss: 0.0130, Domain Loss: 0.6934, Task Accuracy: 0.9922\nEpoch 3, Batch 1600/2916 (54.87%), Task Loss: 0.1076, Domain Loss: 0.6935, Task Accuracy: 0.9688\nEpoch 3, Batch 1700/2916 (58.30%), Task Loss: 0.0135, Domain Loss: 0.6935, Task Accuracy: 1.0000\nEpoch 3, Batch 1800/2916 (61.73%), Task Loss: 0.0492, Domain Loss: 0.6931, Task Accuracy: 0.9922\nEpoch 3, Batch 1900/2916 (65.16%), Task Loss: 0.0115, Domain Loss: 0.6930, Task Accuracy: 1.0000\nEpoch 3, Batch 2000/2916 (68.59%), Task Loss: 0.0341, Domain Loss: 0.6929, Task Accuracy: 0.9844\nEpoch 3, Batch 2100/2916 (72.02%), Task Loss: 0.0654, Domain Loss: 0.6937, Task Accuracy: 0.9844\nEpoch 3, Batch 2200/2916 (75.45%), Task Loss: 0.0085, Domain Loss: 0.6932, Task Accuracy: 1.0000\nEpoch 3, Batch 2300/2916 (78.88%), Task Loss: 0.0535, Domain Loss: 0.6932, Task Accuracy: 0.9844\nEpoch 3, Batch 2400/2916 (82.30%), Task Loss: 0.0099, Domain Loss: 0.6932, Task Accuracy: 1.0000\nEpoch 3, Batch 2500/2916 (85.73%), Task Loss: 0.0489, Domain Loss: 0.6944, Task Accuracy: 0.9922\nEpoch 3, Batch 2600/2916 (89.16%), Task Loss: 0.0128, Domain Loss: 0.6951, Task Accuracy: 1.0000\nEpoch 3, Batch 2700/2916 (92.59%), Task Loss: 0.0381, Domain Loss: 0.6940, Task Accuracy: 0.9766\nEpoch 3, Batch 2800/2916 (96.02%), Task Loss: 0.0349, Domain Loss: 0.6930, Task Accuracy: 0.9922\nEpoch 3, Batch 2900/2916 (99.45%), Task Loss: 0.0074, Domain Loss: 0.6931, Task Accuracy: 1.0000\nEpoch 3 Completed: Task Loss: 0.0423, Domain Loss: 0.6934, Task Accuracy: 0.9877, Time: 1756.46s\nValidation (Overall) - Task Loss: 0.1218, Domain Loss: 0.6968, Task Accuracy: 0.9656\nValidation (Telugu Only) - Task Loss: 0.3051, Domain Loss: 0.6886, Task Accuracy: 0.9322\n","output_type":"stream"}],"execution_count":1}]}