{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ API Key Requirement : \n",
    "> ->To run this code, you must have a valid **GEMINI_API_KEY**.  \n",
    "> ->Visit [Google AI Studio](https://aistudio.google.com/app/apikey) to generate your API key.  \n",
    "> ->Once obtained, specify the **Gemini model** you intend to use for reasoning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T08:01:46.582941Z",
     "iopub.status.busy": "2025-04-01T08:01:46.582700Z",
     "iopub.status.idle": "2025-04-01T08:02:32.640036Z",
     "shell.execute_reply": "2025-04-01T08:02:32.639092Z",
     "shell.execute_reply.started": "2025-04-01T08:01:46.582919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle UserSecretsClient imported successfully.\n",
      "Google Generative AI SDK imported successfully.\n",
      "Gemini API configured successfully with gemini-1.5-flash.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc8d7d45c2e458c960c2372393c1188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c5461e4e1426290b9724104efae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf6a139b66840d88bc514d90ff61370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c68352ca90047a1949c9d87b0f72b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aae6f748844f34b2dbeb9a76f03792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a8eae6212314>:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuRIL DATModel state dictionary loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os  # Added for environment variables if needed, though Kaggle Secrets is preferred\n",
    "\n",
    "# --- Attempt to import Kaggle secrets and google generativeai ---\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    print(\"Kaggle UserSecretsClient imported successfully.\")\n",
    "except ImportError:\n",
    "    UserSecretsClient = None\n",
    "    print(\"Warning: Kaggle UserSecretsClient not found. API key needs to be handled differently if not in Kaggle.\")\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    print(\"Google Generative AI SDK imported successfully.\")\n",
    "except ImportError:\n",
    "    genai = None\n",
    "    print(\"Error: Google Generative AI SDK not installed. Please install using: pip install google-generativeai\")\n",
    "\n",
    "# --- Configure Gemini API (needs to be done once) ---\n",
    "GEMINI_API_KEY = None\n",
    "gemini_model = None\n",
    "\n",
    "if genai is not None:\n",
    "    try:\n",
    "        # Directly input your API key here\n",
    "        GEMINI_API_KEY = \"Your api key here\"\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        # Initialize the specific model we want to use\n",
    "        gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')  # Or 'gemini-1.5-flash'\n",
    "        print(\"Gemini API configured successfully with gemini-1.5-flash.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring Gemini API: {e}. Please ensure GEMINI_API_KEY is set correctly in the code.\")\n",
    "else:\n",
    "    print(\"Cannot configure Gemini API because the SDK is not installed.\")\n",
    "\n",
    "# --- GradientReversal and DATModel class definitions ---\n",
    "class GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class DATModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size=768, num_classes=2, num_domains=2):  # Corrected __init__\n",
    "        super(DATModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task_classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.domain_classifier = nn.Linear(hidden_size, num_domains)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    # forward to handle output_attentions\n",
    "    def forward(self, input_ids, attention_mask, lambda_=1.0, output_attentions=False):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions  # Pass the flag here\n",
    "        )\n",
    "        # features uses the [CLS] token representation (index 0)\n",
    "        features = outputs.last_hidden_state[:, 0, :]\n",
    "        features = self.dropout(features)\n",
    "        task_logits = self.task_classifier(features)\n",
    "\n",
    "        # Domain classification part \n",
    "        domain_features = GradientReversal.apply(features, lambda_)\n",
    "        domain_logits = self.domain_classifier(domain_features)\n",
    "\n",
    "        if output_attentions:\n",
    "            # Return task logits, domain logits, and attentions\n",
    "            return task_logits, domain_logits, outputs.attentions\n",
    "        else:\n",
    "            # Return only task and domain logits\n",
    "            return task_logits, domain_logits\n",
    "\n",
    "# --- Setup device, load tokenizer, base model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_loaded_successfully = False  # Flag to track model loading\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "    base_model = AutoModel.from_pretrained(\"google/muril-base-cased\")\n",
    "\n",
    "    # --- Create DATModel instance and load state dict ---\n",
    "    model = DATModel(base_model).to(device)\n",
    "    # Adjust path as needed\n",
    "    state_dict_path = '/kaggle/input/dat_nlp_model1/pytorch/default/1/model_epoch_3.pth'  # Example path\n",
    "\n",
    "    state_dict = torch.load(state_dict_path, map_location=device)\n",
    "    # Remove 'module.' prefix if present\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(\"MuRIL DATModel state dictionary loaded successfully.\")\n",
    "    model_loaded_successfully = True\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: MuRIL Model state dictionary not found at {state_dict_path}. Ensure the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the MuRIL DATModel state dictionary: {e}\")\n",
    "\n",
    "# --- Function to get prediction and attentions ---\n",
    "def get_prediction_and_attention(text, model, tokenizer, device):\n",
    "    if not model_loaded_successfully or model is None or tokenizer is None:\n",
    "         print(\"MuRIL Model or Tokenizer not loaded. Cannot get prediction.\")\n",
    "         return None, None, None\n",
    "\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,  \n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Request attentions from the model\n",
    "        task_logits, _, attentions = model(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            output_attentions=True  # IMPORTANT: Request attentions\n",
    "        )\n",
    "\n",
    "    _, predicted_class_idx = torch.max(task_logits, 1)\n",
    "    prediction = \"Suicidal\" if predicted_class_idx.item() == 1 else \"Non-Suicidal\"\n",
    "\n",
    "    # Return prediction, encoding dict, and attentions tuple\n",
    "    return prediction, encoding, attentions\n",
    "\n",
    "# --- Function to process attentions and identify important tokens ---\n",
    "def get_important_tokens(encoding, attentions, tokenizer, top_n=5):\n",
    "    \"\"\"\n",
    "    Processes attention weights to find the most attended-to tokens.\n",
    "    This is a simplified approach averaging last layer attentions.\n",
    "    \"\"\"\n",
    "    if encoding is None or attentions is None or tokenizer is None:\n",
    "        print(\"Cannot get important tokens due to missing inputs.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        input_ids = encoding['input_ids'][0]  # Get IDs for the first (only) batch item\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Attentions is a tuple of tensors, one per layer. Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        # We'll focus on the last layer's attentions\n",
    "        last_layer_attentions = attentions[-1].cpu().numpy()  # Shape: (1, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Average attention scores across all heads in the last layer\n",
    "        avg_last_layer_attentions = np.mean(last_layer_attentions[0], axis=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "        # We want to know which input tokens received the most attention overall.\n",
    "        # Sum attention scores directed *to* each token (column sums)\n",
    "        token_attention_scores = np.sum(avg_last_layer_attentions, axis=0)  # Summing across rows (attention *from* all tokens *to* each token `j`)\n",
    "\n",
    "        # Ignore special tokens like [CLS], [SEP], [PAD]\n",
    "        special_tokens = {tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token}\n",
    "        token_scores = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Ensure index is within bounds and token is not special/padding\n",
    "            if i < len(token_attention_scores) and token not in special_tokens and input_ids[i] != tokenizer.pad_token_id:\n",
    "                token_scores.append((token, token_attention_scores[i]))\n",
    "\n",
    "        # Sort tokens by attention score in descending order\n",
    "        token_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the top N tokens (handle potential subword tokens)\n",
    "        important_tokens = [token for token, score in token_scores[:top_n]]\n",
    "\n",
    "        return important_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing attention weights: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Function to generate explanation using Google Gemini API ---\n",
    "def generate_explanation(text, prediction, important_tokens):\n",
    "    \"\"\"\n",
    "    Generates an explanation by prompting the configured Google Gemini model.\n",
    "    (Modified prompt to keep Telugu tokens in the output explanation)\n",
    "    \"\"\"\n",
    "    # Check if Gemini model is configured and available\n",
    "    if gemini_model is None:\n",
    "        print(\"Gemini model not configured or API key missing. Returning placeholder explanation.\")\n",
    "        # Fallback placeholder explanation\n",
    "        important_words_str_placeholder = \" \".join(important_tokens).replace(\" ##\", \"\")\n",
    "        return (\n",
    "            f\"(Placeholder - Gemini API not configured) The model predicted '{prediction}' likely because it focused on: '{important_words_str_placeholder}'. \"\n",
    "            f\"These words might indicate '{prediction}' sentiment based on training data. \"\n",
    "            f\"The model's attention highlights these as key terms.\"\n",
    "        )\n",
    "\n",
    "    # Combine tokens, potentially cleaning up subword indicators like '##'\n",
    "    # For the prompt itself, joining with space is fine.\n",
    "    important_words_str = \" \".join(important_tokens) # Keep subword markers for the prompt list if present\n",
    "   \n",
    "\n",
    "\n",
    "    # ---PROMPT ---\n",
    " \n",
    "    prompt = f\"\"\"The following Telugu text was analyzed by a text classification model (MuRIL-based):\n",
    "Text: \"{text}\"\n",
    "\n",
    "The model predicted the text as: {prediction}\n",
    "\n",
    "Analysis of the model's internal attention mechanism shows it focused heavily on the following tokens/words: \"{important_words_str}\".\n",
    "\n",
    "Based *only* on the text, the prediction ({prediction}), and the fact that the model focused on these specific words, provide a brief explanation (3-4 sentences in English) for why the *original model* likely arrived at its decision.\n",
    "\n",
    "**Crucially, in your English explanation, when you mention the specific important words/tokens from the list provided ({important_words_str}), you MUST write those specific words using their original Telugu script.** For example, instead of saying '...focused on \"last\" and \"day\"...', you should say '...focused on \"చివరి\" and \"రోజు\"...'. Do not translate or transliterate these specific keywords within your explanation text itself.\n",
    "\n",
    "Do not add your own assessment of the text's suicidality; explain the first model's *inferred reasoning* based on its focus points using the original Telugu keywords where appropriate.\n",
    "\"\"\"\n",
    "    # --- END OF PROMPT ---\n",
    "\n",
    "\n",
    "    # --- LLM Interaction using Gemini API ---\n",
    "    try:\n",
    "      \n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            \n",
    "            )\n",
    "\n",
    "        # Extract the text part of the response\n",
    "        explanation = response.text\n",
    "       \n",
    "        return explanation\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini API call: {e}\")\n",
    "        # Return a fallback message indicating the error\n",
    "        return f\"(Error generating explanation via Gemini API: {e})\"\n",
    "    # --- End LLM Interaction ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model inference examples->**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:07:09.924060Z",
     "iopub.status.busy": "2025-03-31T08:07:09.923704Z",
     "iopub.status.idle": "2025-03-31T08:07:10.979693Z",
     "shell.execute_reply": "2025-03-31T08:07:10.978968Z",
     "shell.execute_reply.started": "2025-03-31T08:07:09.924035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నా చివరి రోజు ఇది. ఎవరూ నన్ను ఆపకండి.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['నా', '.', '##పకం', 'చివరి', 'రోజు']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as suicidal because of the phrase \"నా చివరి రోజు,\" meaning \"my last day.\"  The inclusion of  \"ఎవరూ నన్ను ఆపకండి,\"  (\"Don't stop me\"), further reinforces this interpretation. The model's attention to \"##పకం\" (part of \"ఆపకండి\") suggests it recognized the assertive and potentially desperate tone of refusal to be stopped.  The combination of \"నా చివరి రోజు\" and the refusal to be stopped led the model to predict a suicidal intent.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నా చివరి రోజు ఇది. ఎవరూ నన్ను ఆపకండి.\" \n",
    "    # This is my last day. No one should stop me.\n",
    "\n",
    "    # 1. Get prediction and attentions from DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:08:20.460390Z",
     "iopub.status.busy": "2025-03-31T08:08:20.460068Z",
     "iopub.status.idle": "2025-03-31T08:08:21.472256Z",
     "shell.execute_reply": "2025-03-31T08:08:21.471562Z",
     "shell.execute_reply.started": "2025-03-31T08:08:20.460365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: సినిమా చాలా బాగుంది, కథ కూడా చక్కగా ఉంది.\n",
      "Prediction (MuRIL): Non-Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['సినిమా', '.', '##ా', 'బ', '##గు']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as \"Non-Suicidal\" because its attention mechanism prioritized positive sentiment words.  The focus on \"సినిమా\" (movie) and \"బాగుంది\" (good) suggests the model interpreted the overall tone as expressing enjoyment and satisfaction. The presence of  \"చాలా\" (very) amplifies the positive sentiment expressed towards the movie.  Therefore, the absence of negative or self-harming language, coupled with the focus on positive words like  \"సినిమా\" and \"బాగుంది\", led to the \"Non-Suicidal\" classification.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"సినిమా చాలా బాగుంది, కథ కూడా చక్కగా ఉంది.\" \n",
    "    # The movie was really good, and the story was nice too.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:08:32.965530Z",
     "iopub.status.busy": "2025-03-31T08:08:32.965226Z",
     "iopub.status.idle": "2025-03-31T08:08:34.129308Z",
     "shell.execute_reply": "2025-03-31T08:08:34.128529Z",
     "shell.execute_reply.started": "2025-03-31T08:08:32.965508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నా జీవితంలో ఏమీ సరిగ్గా జరగడం లేదు. ఎవరూ నన్ను అర్థం చేసుకోవడం లేదు, నా కష్టాలను ఎవరూ పట్టించుకోవడం లేదు. నాకేమీ ఇష్టం లేదు ఇక సజీవంగా ఉండటానికి.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['##ష్టాలను', 'సరిగ్గా', 'క', '.', '##జీ']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as suicidal due to the negative sentiment expressed throughout.  The model's attention to \"##ష్టాలను\" (kashṭālanu - problems/difficulties) and \"సరిగ్గా\" (sarigga - correctly/properly) highlights the perceived lack of positive aspects in the speaker's life.  The focus on \"##జీ\" (jī - life), within the context of the overwhelmingly negative statements, likely reinforced the model's association with suicidal ideation.  The overall combination of these words, within the context of expressing unhappiness and hopelessness, led the model to its prediction.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text =  \"నా జీవితంలో ఏమీ సరిగ్గా జరగడం లేదు. ఎవరూ నన్ను అర్థం చేసుకోవడం లేదు, నా కష్టాలను ఎవరూ పట్టించుకోవడం లేదు. నాకేమీ ఇష్టం లేదు ఇక సజీవంగా ఉండటానికి.\" \n",
    "    # Nothing is going right in my life. No one understands me, and no one cares about my struggles. I have no interest in staying alive anymore\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:08:45.595335Z",
     "iopub.status.busy": "2025-03-31T08:08:45.595030Z",
     "iopub.status.idle": "2025-03-31T08:08:46.564971Z",
     "shell.execute_reply": "2025-03-31T08:08:46.564155Z",
     "shell.execute_reply.started": "2025-03-31T08:08:45.595313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నాకు చదవడం అంటే చాలా ఇష్టం. కొత్త విషయాలు నేర్చుకోవడం నాకు ఆనందాన్ని ఇస్తుంది. ప్రతిరోజూ కాస్త కాస్తగా కొత్త విషయాలు తెలుసుకుంటూ ముందుకు సాగాలని అనుకుంటున్నాను.\n",
      "Prediction (MuRIL): Non-Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['చదవడం', '.', 'చాలా', '.', 'అంటే']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as Non-Suicidal because the highlighted words \"చదవడం\", \"చాలా\", and \"అంటే\" suggest a positive sentiment.  The phrase indicates a strong liking (\"చాలా ఇష్టం\") for reading (\"చదవడం\"), expressing enjoyment and interest (\"అంటే\"). This positive sentiment, focusing on intellectual pursuits, likely led the model to disregard any potential indicators of suicidal ideation.  The model might have interpreted the overall tone as optimistic and goal-oriented.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నాకు చదవడం అంటే చాలా ఇష్టం. కొత్త విషయాలు నేర్చుకోవడం నాకు ఆనందాన్ని ఇస్తుంది. ప్రతిరోజూ కాస్త కాస్తగా కొత్త విషయాలు తెలుసుకుంటూ ముందుకు సాగాలని అనుకుంటున్నాను.\" \n",
    "    # I love reading. Learning new things makes me happy. I want to keep learning little by little every day and move forward.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:09:07.705767Z",
     "iopub.status.busy": "2025-03-31T08:09:07.705479Z",
     "iopub.status.idle": "2025-03-31T08:09:08.757449Z",
     "shell.execute_reply": "2025-03-31T08:09:08.756744Z",
     "shell.execute_reply.started": "2025-03-31T08:09:07.705744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నా మీద వచ్చిన ఒత్తిడి, బాధ, అనారోగ్యం నాకు మరింత నష్టమే తెచ్చింది. జీవితంలో ఏమీ సరిగ్గా జరగడం లేదు. ఇక ఈ బాధను తట్టుకోలేను.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['ఒత్తిడి', 'మీద', 'వచ్చిన', 'బాధ', 'నా']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as suicidal because the highlighted words \"ఒత్తిడి\", \"మీద వచ్చిన\", \"బాధ\", and \"నా\" strongly suggest overwhelming stress and suffering.  The phrase implies the individual is experiencing intense pressure (\"ఒత్తిడి\") impacting them negatively (\"మీద వచ్చిన\"), resulting in significant distress (\"బాధ\"). The inclusion of \"నా\" (my/me) personalizes the suffering and emphasizes the individual's feeling of being burdened.  The combination of these words created a pattern recognized by the model as indicative of suicidal ideation.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నా మీద వచ్చిన ఒత్తిడి, బాధ, అనారోగ్యం నాకు మరింత నష్టమే తెచ్చింది. జీవితంలో ఏమీ సరిగ్గా జరగడం లేదు. ఇక ఈ బాధను తట్టుకోలేను.\" \n",
    "    # The pressure, pain, and illness I am facing have only brought me more suffering. Nothing is going right in life. I can’t endure this pain anymore\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:09:27.530017Z",
     "iopub.status.busy": "2025-03-31T08:09:27.529682Z",
     "iopub.status.idle": "2025-03-31T08:09:28.607904Z",
     "shell.execute_reply": "2025-03-31T08:09:28.607000Z",
     "shell.execute_reply.started": "2025-03-31T08:09:27.529988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: ఈ రోజు చాలా బిజీగా గడిచింది. ఉదయం నుండి పని లోనే ఉన్నాను, కానీ సాయంత్రం వేళ కాఫీ తాగుతూ విశ్రాంతి తీసుకోవడం ఆనందంగా ఉంది.\n",
      "Prediction (MuRIL): Non-Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['.', 'ఈ', 'చాలా', 'రోజు', '##జీ']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as Non-Suicidal because it identified positive sentiments within the text.  The focus on \"ఈ రోజు\" (this day) and \"చాలా\" (very) suggests the model processed the overall context of the day, noting it was \"బిజీ\" (busy).  The attention given to \"##జీ\" (likely part of \"బిజీ,\" meaning busy), combined with the description of a relaxing evening, likely led the model to conclude that despite a busy day (\"ఈ రోజు\"), there was a positive concluding element, suggesting a non-suicidal state.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"ఈ రోజు చాలా బిజీగా గడిచింది. ఉదయం నుండి పని లోనే ఉన్నాను, కానీ సాయంత్రం వేళ కాఫీ తాగుతూ విశ్రాంతి తీసుకోవడం ఆనందంగా ఉంది.\" \n",
    "    # Today was very busy. I was working since morning, but in the evening, having coffee and relaxing felt good.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:09:48.032641Z",
     "iopub.status.busy": "2025-03-31T08:09:48.032345Z",
     "iopub.status.idle": "2025-03-31T08:09:48.996700Z",
     "shell.execute_reply": "2025-03-31T08:09:48.995957Z",
     "shell.execute_reply.started": "2025-03-31T08:09:48.032613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: అందరూ నన్ను వదిలేసారు నాకు ఒంటరితనం తప్ప మరేమీ లేదు ఇక ఈ బాధలో ఉండలేను.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['.', '##సారు', '##ది', 'వ', 'అందరూ']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as suicidal because of the phrases expressing feelings of abandonment and hopelessness.  The attention on \"అందరూ\" (everyone) and \"##సారు\" (left/abandoned), suggests the model recognized the phrase indicating social isolation. The presence of \"##ది\" (this) likely helped contextualize the following words relating to unbearable suffering, while \"వదిలేసారు\" (left/abandoned) reinforces the sentiment of desertion.  The model's focus on these words points to an interpretation of overwhelming loneliness and despair as expressed in the text.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text =  \"అందరూ నన్ను వదిలేసారు నాకు ఒంటరితనం తప్ప మరేమీ లేదు ఇక ఈ బాధలో ఉండలేను.\" \n",
    "    # Everyone has left me. I have nothing but loneliness. I can’t stay in this pain any longer.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:10:04.249035Z",
     "iopub.status.busy": "2025-03-31T08:10:04.248709Z",
     "iopub.status.idle": "2025-03-31T08:10:05.118369Z",
     "shell.execute_reply": "2025-03-31T08:10:05.117614Z",
     "shell.execute_reply.started": "2025-03-31T08:10:04.249006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నాకు కొత్త ఉద్యోగం వచ్చింది చాలా సంతోషంగా ఉంది కుటుంబంతో సెలవు ప్లాన్ చేస్తున్నాను.\n",
      "Prediction (MuRIL): Non-Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['ఉద్యోగం', 'వచ్చింది', 'కొత్త', '.', 'నాకు']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as Non-Suicidal because the sentence expresses positive sentiment.  The model's attention to \"నాకు కొత్త ఉద్యోగం వచ్చింది\" suggests it recognized the positive news of a new job as a strong indicator of well-being. The presence of  \"నాకు\" further reinforces a sense of agency and positive self-perception. The overall tone conveyed by these words strongly counteracts any indication of suicidal ideation.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నాకు కొత్త ఉద్యోగం వచ్చింది చాలా సంతోషంగా ఉంది కుటుంబంతో సెలవు ప్లాన్ చేస్తున్నాను.\" \n",
    "    # I got a new job and I'm very happy. I'm planning a vacation with my family.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:10:22.984454Z",
     "iopub.status.busy": "2025-03-31T08:10:22.984024Z",
     "iopub.status.idle": "2025-03-31T08:10:24.079848Z",
     "shell.execute_reply": "2025-03-31T08:10:24.079005Z",
     "shell.execute_reply.started": "2025-03-31T08:10:22.984412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నన్ను అందరూ వదిలేసారు నాకు ఈ ఒంటరితనం తప్ప మరేమీ కనిపించడం లేదు ఈ బాధ నుండి బయటపడాలంటే చనిపోవడం తప్ప మరో మార్గం లేదని నమ్ముతున్నాను.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['##ప', '##డా', '##ని', '##తున్న', '.']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as \"Suicidal\" because the phrase containing \"పోవడం\" (pōvaḍaṁ) and \"తున్నాను\" (tuṇṇānu) strongly suggests a belief in death as the only solution.  The model's attention on these words, along with the overall negative sentiment of the sentence, indicates it interpreted the statement as an expression of suicidal ideation. The presence of  \"నమ్ముతున్నాను\" (nammutunnaanu) further reinforces this interpretation, showing a conviction in this belief.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నన్ను అందరూ వదిలేసారు నాకు ఈ ఒంటరితనం తప్ప మరేమీ కనిపించడం లేదు ఈ బాధ నుండి బయటపడాలంటే చనిపోవడం తప్ప మరో మార్గం లేదని నమ్ముతున్నాను.\" \n",
    "    # Everyone has abandoned me, and I see nothing but loneliness. I believe that the only way to escape this pain is to end my life.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:10:35.896053Z",
     "iopub.status.busy": "2025-03-31T08:10:35.895768Z",
     "iopub.status.idle": "2025-03-31T08:10:36.961676Z",
     "shell.execute_reply": "2025-03-31T08:10:36.960868Z",
     "shell.execute_reply.started": "2025-03-31T08:10:35.896031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నాకు ఈ మధ్య కొత్త ఉద్యోగం వచ్చింది దానికి నేను చాలా సంతోషంగా ఉన్నాను కుటుంబంతో కలిసి వచ్చే వారం సెలవుకి వెళ్లాలని ఆలోచిస్తూ టికెట్లు బుక్ చేస్తున్నాను.\n",
      "Prediction (MuRIL): Non-Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['సెలవు', 'ఉద్యోగం', '##ె', '##కి', 'వ']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as Non-Suicidal because the highlighted words \"సెలవు\" (leave/vacation), \"ఉద్యోగం\" (job), and \"##ె ##కి వ\" (likely part of a verb indicating future plans/actions) suggest positive life events.  The mention of a new \"ఉద్యోగం\" and upcoming \"సెలవు\" with family implies happiness and future plans, which are generally associated with a lack of suicidal ideation. The model's focus on the \"##ె ##కి వ\" sequence possibly indicates its recognition of the future tense, further strengthening the positive outlook.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నాకు ఈ మధ్య కొత్త ఉద్యోగం వచ్చింది దానికి నేను చాలా సంతోషంగా ఉన్నాను కుటుంబంతో కలిసి వచ్చే వారం సెలవుకి వెళ్లాలని ఆలోచిస్తూ టికెట్లు బుక్ చేస్తున్నాను.\" \n",
    "    # I recently got a new job, and I am very happy about it. I am planning to go on vacation with my family next week and am booking the tickets.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T03:39:34.726034Z",
     "iopub.status.busy": "2025-04-01T03:39:34.725033Z",
     "iopub.status.idle": "2025-04-01T03:39:36.409199Z",
     "shell.execute_reply": "2025-04-01T03:39:36.408528Z",
     "shell.execute_reply.started": "2025-04-01T03:39:34.726002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: ఇటీవల నాకు కొత్త ఉద్యోగం లభించింది, అందుకు నేను ఎంతో ఆనందంగా ఉన్నాను, కానీ నాకు కుటుంబం లేదు, ఇది కొన్ని సందర్భాల్లో నిరాశాజనకంగా మరియు ఒంటరిగా అనిపిస్తుంది.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['ఇటీవల', 'అందుకు', ',', 'కొత్త', 'ఉద్యోగం']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model's prediction of \"Suicidal\" likely stems from a misinterpretation of the context.  The model may have focused on \"ఇటీవల కొత్త ఉద్యోగం\" (\"recently new job\"), interpreting a significant life change as potentially overwhelming or isolating. The inclusion of \"అందుకు\" (\"for that/because of that\") might have further reinforced this interpretation, suggesting the new job, rather than alleviating loneliness, is somehow contributing to the negative feelings expressed later in the text. The model might have failed to adequately weigh the explicit mention of loneliness and sadness against the seemingly positive context of a new job.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"ఇటీవల నాకు కొత్త ఉద్యోగం లభించింది, అందుకు నేను ఎంతో ఆనందంగా ఉన్నాను, కానీ నాకు కుటుంబం లేదు, ఇది కొన్ని సందర్భాల్లో నిరాశాజనకంగా మరియు ఒంటరిగా అనిపిస్తుంది.\" \n",
    "    # Recently, I got a new job, and I am very happy about it, but I don't have a family, which feels disappointing and lonely at times.\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T08:02:45.830204Z",
     "iopub.status.busy": "2025-04-01T08:02:45.829831Z",
     "iopub.status.idle": "2025-04-01T08:02:47.590956Z",
     "shell.execute_reply": "2025-04-01T08:02:47.590069Z",
     "shell.execute_reply.started": "2025-04-01T08:02:45.830177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Input Telugu Text  ---\n",
      "Input: నేను బ్రతకాలని అనుకోవడం లేదు, కానీ నా కుటుంబం కోసం బ్రతుకుతున్నాను.\n",
      "Prediction (MuRIL): Suicidal\n",
      "Identified Important Tokens (MuRIL Focus): ['.', '##తున్న', ',', 'కానీ', '##ుకు']\n",
      "\n",
      "Explanation (Generated by Gemini based on MuRIL focus):\n",
      "The model likely classified the text as \"Suicidal\" because of the phrase \"నేను బ్రతకాలని అనుకోవడం లేదు,\" which expresses a lack of desire to live.  The model's attention on \"##తున్న\" and \"కానీ ##ుకు\" suggests it interpreted the contrasting phrase \"కానీ నా కుటుంబం కోసం బ్రతుకుతున్నాను\" as a reason for continued living despite the initial expression of not wanting to live, highlighting the conflict and potentially associating it with suicidal ideation.  The model may have focused on these specific parts to identify the internal conflict and the statement implying lack of will to live.\n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_loaded_successfully and tokenizer is not None: # Proceed only if MuRIL model loaded\n",
    "\n",
    "    telugu_text = \"నేను బ్రతకాలని అనుకోవడం లేదు, కానీ నా కుటుంబం కోసం బ్రతుకుతున్నాను.\" \n",
    "    # I don't want to live, but I am living for my family\n",
    "\n",
    "    # 1. Get prediction and attentions from  DATModel\n",
    "    prediction, encoding, attentions = get_prediction_and_attention(telugu_text, model, tokenizer, device)\n",
    "\n",
    "    if prediction is not None: # Check if prediction was successful\n",
    "        # 2. Identify important tokens based on attention\n",
    "        important_tokens = get_important_tokens(encoding, attentions, tokenizer, top_n=5)\n",
    "\n",
    "        # 3. Generate the explanation using Gemini API\n",
    "        # The function now directly uses the configured gemini_model\n",
    "        explanation = generate_explanation(telugu_text, prediction, important_tokens)\n",
    "\n",
    "        # 4. Print results\n",
    "        print(f\"\\n--- Results for Input Telugu Text  ---\")\n",
    "        print(f\"Input: {telugu_text}\")\n",
    "        print(f\"Prediction (MuRIL): {prediction}\") # Use the obtained prediction\n",
    "        print(f\"Identified Important Tokens (MuRIL Focus): {important_tokens}\")\n",
    "        print(f\"\\nExplanation (Generated by Gemini based on MuRIL focus):\")\n",
    "        print(explanation)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 280209,
     "modelInstanceId": 259011,
     "sourceId": 303374,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
